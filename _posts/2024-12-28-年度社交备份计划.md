最近两年到了年底总习惯备份一下自己的社交网站数据，一方面以时间线的形式集中整理一下自己的社交活跃记录，另一方面也防止哪天被封号，临近新年有时间，遂坐在家中写下了今年的备份方法，仅供参考。

[整体思路](../media/pic/backup.png)





##### 1.范围  
目前还在发表观点的网站主要有豆瓣，微博，instagram，youtube，bilibili，微信，QQ，twitter。
 - 其中微信和QQ数据不适宜放在blog里，因此暂不考虑
 - youtube和bilibili发布的内容一致，且一年一更，可以简单的手动备份一下
 - twitter在2019年后彻底没更新过，原因是免费的ifttt服务去掉了weibo渠道，本年度备份可以不考虑，需要找个时间再度和weibo同步一下
 - instagram去年也没有发布过内容， out
 - 综上，剩下的可供自己备份的网站其实只有豆瓣和微博

##### 2.导出。  
秉持着不重复造轮子的原则，先在网上找了一圈已有方案:

###### 豆瓣主要导出[书影音说]  
1. 通过广播api导出所有记录，api已失效  
接口地址：https://api.douban.com/shuo/v2/statuses/user_timeline/{id}  
2.通过book api导出所有看过和想看的记录，api可用，也有上古神key流传下来
接口地址:https://api.douban.com/v2/book/user/{userid}/collections?apikey={apikey}
3.通过collection api导出用户所有书影音记录，api不可用,报错为apikey is required. Please contact bd-team@douban.com for authorized access.  
接口地址:https://api.douban.com/v2/user/{userid}/collections?apikey={apikey}  

此时只想感叹一句互联网精神已死，连豆瓣这种一开始就高举分享大旗的网站也变得如此封闭，之后只好找找爬虫有什么轮子。  
1. DouBanSpider可用，数据导出较为粗糙 https://github.com/Python3Spiders/DouBanSpider
2. douban_crawler可用，数据比较详细，输出为csv https://github.com/JimSunJing/douban_crawler
3. 豆瓣逃离计划，需要装油猴脚本，并且支持导出到notion https://ulyc.github.io/2022/02/11/Douban-Escape-Plan/
4. 豆瓣标记导出到notion, 上一个项目的源头，https://zhuzi.dev/posts/2021-06-05-douban-backup-sync-notion/

三号和四号方案里提到可以用github action定时从rss更新，这个不用考虑爬虫方案里的机器人识别ip封锁的问题，感觉可以留个坑明年再折腾。最终douban导出选了方案1的python，稍加修改成功生成出来一个包含2022-03-06(上次备份时间)之后记录的json文件。  

每个标记的项目格式如下:  
```json
{
  "created_at": "2024-05-05 12:01:54",
  "status_type": "movie",
  "status_url": "https://www.douban.com/people/superz/status/4599113495/",
  "status_text": "废话\n      看过",
  "status_resource_title": "东京大饭店 グランメゾン東京‎ (2019)",
  "status_resource_url": "https://movie.douban.com/subject/33464695/"
}  
```
###### 有了豆瓣的经验，微博导出直接去找了爬虫方案
weibo-crawler项目说明详细而且还在更新，只要配置好自己的id和上次同步时间(2022-01-14)即可， [weibo-crawler](https://github.com/dataabc/weibo-crawler)

导出的json文件里会用到的字段主要有 created_at | original_pics | video_url |  text

图片链接直接作为外链使用会报错403，因此加了前缀用百度做图床`https://image.baidu.com/search/down?url=`  

至此导出工作全部完成。

##### 3.转换
接下来需要将json里每个对象生成.md文件给github page使用,

###### 豆瓣md格式:

```
---
duplicate json field
date: created_at
tags: [status_type]
title: status_text - status_resource_title
---
[豆瓣链接](resource_url)


```

###### 微博md格式:
```
---
duplicate json field
layout: post
date: created_at
title: 发表微博
---
![](pics | video_url)
text
```

已知存在的问题留给2025年修复，
1. 如果微博一次发表了多个照片，在.md里只能提取到第一个
2. 如果微博发表的是视频，则由于403报错无法展示
3. 豆瓣无法展示自己对标记的打分和评价
4. 转化后的.md格式不是很统一，豆瓣缺少layout，微博缺少tag，二者都缺少category

##### 4.发布
github page使用的是jekyll模版，将新生成的md文件存放之step文件即可。



##### 5.UI更新
今年对timeline的css稍微调教了下，字体和框架都往小缩放了一倍，让timeline顺眼了点，最终效果如下
[](../media/pic/timeline.png)  


总体来说，这次备份时间主要花在了数据导出上，希望这个一劳永逸的方案不要被各大平台封锁掉，目前每年一更新，而且爬虫方案还要求提供cookie，对自动化的需求也不是很强烈，暂时先将脚本全部统一为python，留下的坑明年看看是否有更好的方案来简化吧。
